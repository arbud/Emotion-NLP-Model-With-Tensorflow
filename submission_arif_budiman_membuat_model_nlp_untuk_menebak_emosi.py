# -*- coding: utf-8 -*-
"""Submission Arif Budiman - Membuat model NLP untuk menebak emosi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yFnj2iuO3tJq03tC1KOI9t6oTuzuUqtT

# **Pendahuluan**

Pertama-tama dalam proyek ini saya menggunakan dataset yang diperoleh dari [kaggle](https://www.kaggle.com/) yaitu Emotions datasets for NLP dari link sebagai berikut : https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp/data

Data yang terdapat dari link tersebut kemudian saya olah lagi menggunakan python dan memberikan hasil data sebagai berikut yang sudah saya upload ke google drive : https://drive.google.com/file/d/1fSfIh1EaI-o9B0hIqGhCxb-8q5qZKyCk/view?usp=drive_link

Tujuan dari proyek ini adalah dapat membangun model NLP untuk mengetahui emosi berdasarkan teks pada data yang telah disediakan.

# **Import Libraries**
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import sklearn
import matplotlib.pyplot as plt
import time
from tensorflow import keras
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense

# Mengambil dataset dari link google drive yang telah dijelaskan pada pendahuluan
# Untuk datasetnya sebelumnya sudah di share dan untuk mengambilnya dapat dilihat pada linknya diantara d/ dan sebelum /view

# Ganti 'your_file_id' dengan ID berkas Google Drive Anda
file_id = '1fSfIh1EaI-o9B0hIqGhCxb-8q5qZKyCk'

# URL untuk file CSV di Google Drive
url = f'https://drive.google.com/uc?id={file_id}'

# buat dataframenya dari url
df_emosi = pd.read_csv(url, names=['kalimat', 'ekspresi'], sep=';')
df_emosi

# melihat info dataframenya
df_emosi.info()

#dapat dilihat bahwa jumlah data ada sekitar 20.000 teks

# Menambahkan kolom 'Jumlah_Kata_Terpanjang' ke DataFrame yang berisi jumlah kata paling panjang setiap kalimat
df_emosi['Jumlah_Kata_Terpanjang'] = df_emosi['kalimat'].apply(lambda x: len(str(x).split()))

# Menemukan jumlah kata paling panjang
jumlah_kata_terpanjang = df_emosi['Jumlah_Kata_Terpanjang'].max()

# Menampilkan hasil
print("Jumlah kata paling panjang:", jumlah_kata_terpanjang)

# mengetahui data pada kolom ekspresi yang akan dijadikan label nantinya
df_emosi['ekspresi'].unique()

# diketahui ekspresi terbagi menjadi 6 macam

# lakukan one-hot encoding pada kolom ekspresi dan buat dataframe baru
categoryekspresi = pd.get_dummies(df_emosi.ekspresi)
dfemosi_baru = pd.concat([df_emosi, categoryekspresi], axis=1)
dfemosi_baru = dfemosi_baru.drop(columns='ekspresi')
dfemosi_baru

# drop kolom yang tidak dibutuhkan seperti jumlah_kata_terpanjang
dfemosi_baru = dfemosi_baru.drop(columns='Jumlah_Kata_Terpanjang')

dfemosi_baru

# Ubah dataframe menjadi array
kalimatarray = dfemosi_baru['kalimat'].values
ekspresiarray = dfemosi_baru[['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']].values

kalimatarray

ekspresiarray

# bagi datanya menjadi data train dan data validation dimana persentase nya adalah 20%
kalimatarray_train, kalimatarray_val, ekspresiarray_train, ekspresiarray_val = train_test_split(kalimatarray, ekspresiarray, test_size=0.2)

# Mengetahui total data train dan validasi

print('total kalimat train : ', len(kalimatarray_train))
print('total kalimat val : ',len(kalimatarray_val))
print('total ekspresi train : ', len(ekspresiarray_train))
print('total ekspresi val : ',len(ekspresiarray_val))

# Lakukan tokenizer dan pad_sequence
tokenizerekspresi = Tokenizer(num_words=50000, oov_token='-')
tokenizerekspresi.fit_on_texts(kalimatarray_train)

sequence_train = tokenizerekspresi.texts_to_sequences(kalimatarray_train)
sequence_val = tokenizerekspresi.texts_to_sequences(kalimatarray_val)

paddedtrain_ekspresi = pad_sequences(sequence_train,
                                     padding='post',
                                     maxlen=100,
                                     truncating='post')

paddedval_ekspresi = pad_sequences(sequence_val,
                                     padding='post',
                                     maxlen=100,
                                     truncating='post')

print(tokenizerekspresi.word_index)

# buat arsitektur model dengan Embedding dan LSTM dengan input_dim = 50.000 ,output_dim = 200, dan input_length = 100
modelekspresi = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim = 50000, output_dim = 200, input_length=100),
    tf.keras.layers.LSTM(64, return_sequences=True, batch_input_shape=(128, 100, 200)),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(6, activation='softmax')
])

modelekspresi.compile(loss='categorical_crossentropy',
                      optimizer='Adam',
                      metrics=['accuracy'])

# buat callback untuk mengetahui jika akurasi sesuai dengan yang ditentukan
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

# ujicoba model yang telah dibuat di atas
start = time.time()
historyekspresi = modelekspresi.fit(paddedtrain_ekspresi, ekspresiarray_train, epochs=50,
                                    validation_data=(paddedval_ekspresi,ekspresiarray_val), verbose=2, callbacks=[callbacks])
stop = time.time()
print(f"Lama Waktu Training yang Dibutuhkan: {round((stop - start)/60)}minute")

# Buat Grafik Untuk Mengetahui akurasi model yang telah dilatih dengan matplotlib

acc = historyekspresi.history['accuracy']
val_acc = historyekspresi.history['val_accuracy']

loss = historyekspresi.history['loss']
val_loss = historyekspresi.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Categorical CrossEntropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()